{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/posts/ml-project","result":{"data":{"markdownRemark":{"id":"aa46d7a0-8194-5053-86bb-db91e8a1706e","html":"<p>This is the webpage of WPI CS 539 (Machine Learning) final project.</p>\n<p>Group Member:</p>\n<p>Jiaming Nie, Ruojun Li, Yu Li, Yang Tao, Guangda Li</p>\n<h1 id=\"introduction-to-style-transfer\" style=\"position:relative;\"><a href=\"#introduction-to-style-transfer\" aria-label=\"introduction to style transfer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction To Style Transfer</h1>\n<p>Style transfer is the technique of recomposing images in the style of other images. Three images will be taken, a content image, a style image and output image.</p>\n<p>Deep convolutional neural network is a powerful tool to extract the feature which represents the styles. The style could be the structure of one painting, the way artists draw their paintings and the colors that the artists use. Representation of content images can also be extracted using the deep convolutional neural network.</p>\n<p>A style transfer demo is illustrated on the following image:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d068b09179d2fac329ed722c8820855c/c58a3/uol_output.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.083333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQBAgP/xAAVAQEBAAAAAAAAAAAAAAAAAAABA//aAAwDAQACEAMQAAABWhwpKpiJ/8QAGhAAAwEAAwAAAAAAAAAAAAAAAAECEhETMf/aAAgBAQABBQLdcJDls6ZRkv3/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAXEQEAAwAAAAAAAAAAAAAAAAAAAREh/9oACAECAQE/AcVL/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAEQMREhkf/aAAgBAQAGPwIxtlcRbi2f/8QAHBAAAgIDAQEAAAAAAAAAAAAAAAERITFRgWGR/9oACAEBAAE/IcBRw0HcH1UFMJqV+jScvZJV+x//2gAMAwEAAgADAAAAEHDf/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARYf/aAAgBAwEBPxDUh5f/xAAYEQACAwAAAAAAAAAAAAAAAAAAARExUf/aAAgBAgEBPxCWpGLP/8QAGxABAQADAQEBAAAAAAAAAAAAAREAIUExgfD/2gAIAQEAAT8QNGdooRBvMeQRofWCViAKJ9ncNcgv4YjkFBr2YWgScx//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/d068b09179d2fac329ed722c8820855c/8ac56/uol_output.webp 240w,\n/static/d068b09179d2fac329ed722c8820855c/d3be9/uol_output.webp 480w,\n/static/d068b09179d2fac329ed722c8820855c/e46b2/uol_output.webp 960w,\n/static/d068b09179d2fac329ed722c8820855c/f992d/uol_output.webp 1440w,\n/static/d068b09179d2fac329ed722c8820855c/293e0/uol_output.webp 1500w\"\n              sizes=\"(max-width: 960px) 100vw, 960px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/d068b09179d2fac329ed722c8820855c/09b79/uol_output.jpg 240w,\n/static/d068b09179d2fac329ed722c8820855c/7cc5e/uol_output.jpg 480w,\n/static/d068b09179d2fac329ed722c8820855c/6a068/uol_output.jpg 960w,\n/static/d068b09179d2fac329ed722c8820855c/644c5/uol_output.jpg 1440w,\n/static/d068b09179d2fac329ed722c8820855c/c58a3/uol_output.jpg 1500w\"\n            sizes=\"(max-width: 960px) 100vw, 960px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/d068b09179d2fac329ed722c8820855c/6a068/uol_output.jpg\"\n            alt=\"uol output\"\n            title=\"uol output\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h1 id=\"images-representation\" style=\"position:relative;\"><a href=\"#images-representation\" aria-label=\"images representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Images Representation</h1>\n<h2 id=\"use-pre-trained-cnn-to-generate-activation-maps\" style=\"position:relative;\"><a href=\"#use-pre-trained-cnn-to-generate-activation-maps\" aria-label=\"use pre trained cnn to generate activation maps permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Use Pre-trained CNN to generate activation maps</h2>\n<p>Deep convolutional neural network can output the activation map, which are features representing the content or style of an image. In this project, a ImageNet pretrained vgg16 network will be used to output the feature at different layers.</p>\n<p>VGG 16 is a deep CNN including 5 convolutional layers, 2 fully connetced layers and 1 softmax output layer. In this project, only the convolutional layers will be used as they will output the activation maps.</p>\n<p>The architecture of VGG16 is on the following:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/db1c097ad63b388bcd303d0a65fee384/5a190/vgg16.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 23.333333333333332%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA4klEQVQY003G226EIBAAUP//96xtom0WxV0Lg4Ay3BSYJu1Lz9Pp+r7nnJdaa/S4K2qUWwYPRCReLvjLGh98jrW4+66l5F+1ViLqvNiC0fm+KaeW0+V9bOm8D9Agn+fMVpB2YesR40trpVSMERFLKSGEzp0HSGmteS7z+NEvjG3b1g9voBQRlVIaUa11GkdjrJSCc76uqxCSsblDROec+BaL4NP2We5iDxyGrytn+sdaq7W2x/EXRPTed/u+G2Naa+482eMhAWKMw/AupTTGhBC01jElRDeOEwAopQAgpQSgfgBZ2RlQrLc4dAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/db1c097ad63b388bcd303d0a65fee384/8ac56/vgg16.webp 240w,\n/static/db1c097ad63b388bcd303d0a65fee384/d3be9/vgg16.webp 480w,\n/static/db1c097ad63b388bcd303d0a65fee384/d00b9/vgg16.webp 800w\"\n              sizes=\"(max-width: 800px) 100vw, 800px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/db1c097ad63b388bcd303d0a65fee384/8ff5a/vgg16.png 240w,\n/static/db1c097ad63b388bcd303d0a65fee384/e85cb/vgg16.png 480w,\n/static/db1c097ad63b388bcd303d0a65fee384/5a190/vgg16.png 800w\"\n            sizes=\"(max-width: 800px) 100vw, 800px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/db1c097ad63b388bcd303d0a65fee384/5a190/vgg16.png\"\n            alt=\"vgg16\"\n            title=\"vgg16\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>VGG16 takes 224 as the input size and all the convolutional layers are hidden layers.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 790px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3d3d1764d8e9bbff8810f2631bc8ff58/2e237/layer2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.75000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAACTklEQVQozwXBzU/TUAAA8Eb9A4xnI2LUA4FoZH6gISpOBuyzbVz72r73+rW2761byxgtMNhgCIjA+BBRyAImBExIjAejN2OiiV68TIlXj/4FnvX3Y8ycIwBJwVg3c48EwbSsLAAQIULzlgp7urvjhrpYxV+P0Pakm4vetNCgS8QKlxw43874I4Gmmw6lRX8Ya1qpPKoZpkPoeDjGy+KtaOJSjKMw/iEf23w80mxUSyFFkbZVKt/t6mQQ1liOzwqCAlGa5bCmsTwviEA3TB0q7I2LD7lEJjXUAFk82FuxE4qa1vuiCxOF+IMI41kUibJqGLZDBCDZrishaOi6Qb2NUblltn/MR/28vrw4V/NLRMczgT1dDbbX5vr77jAFx0WirJumQ4ggScR1ZQgNw8iR4vqU9c3reqdfWaqPLFTK808XxsJwujY76QprE0rkai+DEWZZLisDBaNkOg0hzLCcKAIgyctLKzu7rzYOjm6XwzOXOyPRISUTywz0V3yjsVIGQGY8v6TqupWnvu9BhIqep2qaTfOU5l80m28+tb58/9U23zx1LdZz5fpA7B6fStan7GcbczMzdUYEUiKZEiQRYZhiMwhjjucFIGFVLQXjm89f7u7tcU8cjnaAobOaKFmWVnAhez9eKgdMtVZzCNEN1bJNAQCLUEmWsabZDhkNJ2SoIo183j/978+Jw52O9Ur49qBRUNX1SskxMNNq/fh5fByMlR2agxgV/GGEVcuyi54fBgHEkqDA94fn/v4++Xr1wv7W0lYdphKx5dkpYuD/XL3zSklcktYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/3d3d1764d8e9bbff8810f2631bc8ff58/8ac56/layer2.webp 240w,\n/static/3d3d1764d8e9bbff8810f2631bc8ff58/d3be9/layer2.webp 480w,\n/static/3d3d1764d8e9bbff8810f2631bc8ff58/02aff/layer2.webp 790w\"\n              sizes=\"(max-width: 790px) 100vw, 790px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/3d3d1764d8e9bbff8810f2631bc8ff58/8ff5a/layer2.png 240w,\n/static/3d3d1764d8e9bbff8810f2631bc8ff58/e85cb/layer2.png 480w,\n/static/3d3d1764d8e9bbff8810f2631bc8ff58/2e237/layer2.png 790w\"\n            sizes=\"(max-width: 790px) 100vw, 790px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/3d3d1764d8e9bbff8810f2631bc8ff58/2e237/layer2.png\"\n            alt=\"layer2\"\n            title=\"layer2\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Layer 2 of the CNN. The left image represents what the CNN has learned and the right image has parts of actual images.</p>\n<ul>\n<li>In layer 2 of the CNN the model is already picking up more interesting shapes than just diagonal lines. In the sixth square (counting horizontally) you can see that the model is picking up circular shapes. Also, the last square is looking at corners.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f9dcb43e3297f438b3c71729c2a96d5a/5a190/layer3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.083333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfklEQVQY0yXPz2oTURQG8HkeQXAh1IW4Ci4aahOodDqm06ZRpjV/Zub8uffc6WSa2Ka5GWk0pCZmIWQhgkhwpfgIPkbIk8hE+PGd3fdxHBajTUJKKa1ZFYeVRsUqMUm3G/ruQenBW/fl3z+/pxMLdDRML87DslwfRdmhE5MiVgAASDEgYCEGQFbGJPs7j1yv8i6xX+fLxceb5ptW3fNf7D11a8/tCBylRcTgtoKYt6mISIu5Si/Hx4/vMwVR+/Bgt3NaTU3c63LtNbpnyWr10xFttJgIEIkBmagAiCySiqx2HnbPG6XybsOr5+pkYifff6xaTWkHz34Nmk4YxVEMEAMhAxIWGACL5410qk9MO8gwMH7lNijb695inAXH+516aXkTOvP54tNsJiYhYNouIylA1mIkueyZMGf/vZx+vqh+069Gfb202YepDeqVu37L2Ww26/X6fjYb3A6tHf03HNo8z8d340FGJzVvwI22vze9UlnD+5JKvxeHHb8v0T9ZHsKA5kRENwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/f9dcb43e3297f438b3c71729c2a96d5a/8ac56/layer3.webp 240w,\n/static/f9dcb43e3297f438b3c71729c2a96d5a/d3be9/layer3.webp 480w,\n/static/f9dcb43e3297f438b3c71729c2a96d5a/d00b9/layer3.webp 800w\"\n              sizes=\"(max-width: 800px) 100vw, 800px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/f9dcb43e3297f438b3c71729c2a96d5a/8ff5a/layer3.png 240w,\n/static/f9dcb43e3297f438b3c71729c2a96d5a/e85cb/layer3.png 480w,\n/static/f9dcb43e3297f438b3c71729c2a96d5a/5a190/layer3.png 800w\"\n            sizes=\"(max-width: 800px) 100vw, 800px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/f9dcb43e3297f438b3c71729c2a96d5a/5a190/layer3.png\"\n            alt=\"layer3\"\n            title=\"layer3\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ul>\n<li>In layer 3 we can see that the model is starting to learn more specific things. The first square shows that the model is now able to recognize geometrical patterns. The sixth square is recognizing car tires. And the eleventh square is recognizing people.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/538599044d8f9033312285511b290641/00d43/layer5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 24.166666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAIAAADKYVtkAAAACXBIWXMAAAsTAAALEwEAmpwYAAABHUlEQVQY0xXMy07CQBQA0P6bC1dGTYgYJQVFYguMtLVApzP3ztB2Rq0VShoITUhkq7Al6sZX9L+M5wOOIaQEQAAhRzHnvtW+HPgudT3gLC9mQUAZg5BxHrJkUpDOycA6stqHpNt6Ws2NKFZSjuQoSpQe+AST1O3Ti5aptMxnJeMMhQD83/OJjmk98erXbq1e2U3TzEAhAZEDRrES/jmxTdR6ODzVcWOcxjRkjHPOIQhZOc6nkbZr+7Jfc5sH2/dXQyklhBAooyhR4FBSzTI2lkF2dZzfAkYJADLAkPF8OvW8TqOyR60qIebby7Px8fVZLkoppL65K1IV9M6k3Xz07Xve3a7X3z+/m80GUCCKYl46jueQ9gM3nd7OcrX8A4LmlAFmzd6tAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/538599044d8f9033312285511b290641/8ac56/layer5.webp 240w,\n/static/538599044d8f9033312285511b290641/d3be9/layer5.webp 480w,\n/static/538599044d8f9033312285511b290641/e46b2/layer5.webp 960w,\n/static/538599044d8f9033312285511b290641/a5d4d/layer5.webp 1000w\"\n              sizes=\"(max-width: 960px) 100vw, 960px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/538599044d8f9033312285511b290641/8ff5a/layer5.png 240w,\n/static/538599044d8f9033312285511b290641/e85cb/layer5.png 480w,\n/static/538599044d8f9033312285511b290641/d9199/layer5.png 960w,\n/static/538599044d8f9033312285511b290641/00d43/layer5.png 1000w\"\n            sizes=\"(max-width: 960px) 100vw, 960px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/538599044d8f9033312285511b290641/d9199/layer5.png\"\n            alt=\"layer5\"\n            title=\"layer5\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ul>\n<li>Finally, layers 4 and 5 continue this trend. Layer 5 is picking up tings that are very useful for our dogs and cats problem. It is also recognizing unicycles and bird/reptile eyes. Be aware that these images only show a very small fraction of things learned by each layer.</li>\n</ul>\n<p>As the network went deeper, the features became more high-level and abstrated for human to recongnize. The pretrained vgg16 will be able to filter out the objects in one image.</p>\n<h2 id=\"representation-of-content\" style=\"position:relative;\"><a href=\"#representation-of-content\" aria-label=\"representation of content permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Representation of Content</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 683px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d20eef1a53fd3fa204018097dd48d381/bca35/rep_content.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 78.75000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACFElEQVQ4y3VUa5OjIBD0//+8+3DZut27xPKJiiAIKAh9BSYxj92hKB7ONE0PY4YfLISQRuccWtKiqioURQHG+f17bK+WPQY/jrH74CFmkcAueY6maWCUefJ5jcm+A7odHKzHmA/QRsMsBvM8w3Urgt3ZfUcke928zSWXEP0Ep9x183rOBgTl36TZiXzDkBCCf+cz1KxQ1XVidgQgMZvljLqukec5KKXX+HAwvAGOjCeHqFlMRtd3O8iDz7ZtKMoCv08f6fC+bwH4Q8MDHcjPJww9wSQE+r7H2P19y3z0a9sG569fIKQD7S7wdjqyfGgXYFUDhF2fZVnAaAMpJ0gp4Zx9QLZwy3hfCsHA2AjG2A7ofYBRHHZdkoMxGh3pMHGRMhufi3XuDrA5jRC2NI96cjbhkl9QliWy6Bi1GEcK0pEdUBu0tEE/EXA+wZgjMZxzDEMPzhm0MqBzj5oWkGKG0goZHWlKxDiO6Zo3M15Bb/JJwwhcViX6YQAdKIQUmBzFGo64TGuNxawY5QBh2XEt77Bs5gmwkQUaXmI1FlwwSDchXDWP471SbqbdDOctFrOATzwFRdZxvTgDs6nkt3oD4+efK+Wx3PSqkqbxUceEDMOAqq6grIT1K2YrdubhvVJie3rYwQcIIZNWEezP5ydOHx+QUsBjS0DHFd9r+e1v47YNQghEXWNvCQHj7F7Lr/X7Oo+A/wGxCOB4vsLpJwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/d20eef1a53fd3fa204018097dd48d381/8ac56/rep_content.webp 240w,\n/static/d20eef1a53fd3fa204018097dd48d381/d3be9/rep_content.webp 480w,\n/static/d20eef1a53fd3fa204018097dd48d381/e2d2f/rep_content.webp 683w\"\n              sizes=\"(max-width: 683px) 100vw, 683px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/d20eef1a53fd3fa204018097dd48d381/8ff5a/rep_content.png 240w,\n/static/d20eef1a53fd3fa204018097dd48d381/e85cb/rep_content.png 480w,\n/static/d20eef1a53fd3fa204018097dd48d381/bca35/rep_content.png 683w\"\n            sizes=\"(max-width: 683px) 100vw, 683px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/d20eef1a53fd3fa204018097dd48d381/bca35/rep_content.png\"\n            alt=\"rep content\"\n            title=\"rep content\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>The structure of the content image could be easily extracted using the output of the last convolutional layer of vgg16.</p>\n<h2 id=\"representation-of-style\" style=\"position:relative;\"><a href=\"#representation-of-style\" aria-label=\"representation of style permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Representation of Style</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 746px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/986fc3f1ebbd6c5d5803aab6c1ab8807/62de4/rep_style.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.916666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNUlEQVQoz21Sga6EIAzz/7/TGL1TkIGoeNqXzuzi8yRZEFa6rrM6jgP3tW0bppiQ0wTnHPI86/0dy/O6rogxYooT6rpGxcTn89EEo5SCIAI3OOSUMY6j5o2gbOWLXZYFgxtOQkno+/4ktOQ8z/9I/egRQsC+719CYohlrOV8N4ZRg9jKWiSYO9WwUkoJKSe0Xav3RmgYBsler5eK8OLRNM1JyMQ0Tcg5w85slUEPr54RQyxJuCRGVTYMZ+sVQeYR96ch2R1b1zj2rw33VdmkqIrVeX6K6/DMayt2jcp8MW/s8ZXoSsiBEUuF97wq5HToA/vnt3l2B5uvxEkUxXFwP4TBBywyow89Gtfo//ekkKbTErbbtDW6d/tMKCKQIAgS0L07fXAnZKvee1VGhc47LVDK9kP4BxhlXQlsXBggAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/986fc3f1ebbd6c5d5803aab6c1ab8807/8ac56/rep_style.webp 240w,\n/static/986fc3f1ebbd6c5d5803aab6c1ab8807/d3be9/rep_style.webp 480w,\n/static/986fc3f1ebbd6c5d5803aab6c1ab8807/f7ebd/rep_style.webp 746w\"\n              sizes=\"(max-width: 746px) 100vw, 746px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/986fc3f1ebbd6c5d5803aab6c1ab8807/8ff5a/rep_style.png 240w,\n/static/986fc3f1ebbd6c5d5803aab6c1ab8807/e85cb/rep_style.png 480w,\n/static/986fc3f1ebbd6c5d5803aab6c1ab8807/62de4/rep_style.png 746w\"\n            sizes=\"(max-width: 746px) 100vw, 746px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/986fc3f1ebbd6c5d5803aab6c1ab8807/62de4/rep_style.png\"\n            alt=\"rep style\"\n            title=\"rep style\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Similarly, the style of one image could also be extratced using the same method.</p>\n<h2 id=\"feature-and-gram-matrix\" style=\"position:relative;\"><a href=\"#feature-and-gram-matrix\" aria-label=\"feature and gram matrix permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature and Gram Matrix</h2>\n<p>The activation map is a combined matrix which are high-level features human can easily recongnize.</p>\n<p>In style transfer, network need to convert them into another shape for the traning and testing procedure.</p>\n<ul>\n<li>First, convert the activation map into feature matirx.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 475px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3d1d42e258bf0bcd10b07e2b626101b9/466da/feature_matrix.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABsUlEQVQoz32Si46bMBBF8/9ft1VRok2lJCwhvLIUsPEDbJ/KRptupLQjWWMzD+7cO7sQAq9OtNvtRp7nFNeCw+HA5XLhu33lffe7/zWs65q2a6mbmo+iIP/IsdbivccYk/KWZXmq2fHCAuFx7+5d8rFR/H4+nwnBc/x1xDlHlmVPtbtxHIlnGEby/IOqqlmXNQW11rz9eEujF0VBVdfs93v6vudnlnGrbpRlSdu2dF3HrarYDcPAtSwTT6fiTPPZ0MseqSSn0wk5S4QQjGJinEYmORFrZjWn9zANCVDMWdf178jSSo7NkWP9TlZmlPeSoR9wOOxiMYtBG51iyij0olMDs9rkHyOHJ8bABx9JRCtN3TSJbCllEiGiMKthEgKlFdM0JVGElCilEs+7sAbcp8WPC66zuKvGlxbVCpquAb8JspoFbQy+suhB4VaH0ZrgQopHgTaEPuBnRzAer1y6YwJKKJp2axgRWmMRUrDcNeK3wOgNsdWWWc5JwIj25dokhY2maZqNXylTcvQOn4SKqOI78jfPW8Mkyr+WOibExY72tcTRP3YybKNGiz+Lyx5H/gMFQFQyzN5+2QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/3d1d42e258bf0bcd10b07e2b626101b9/8ac56/feature_matrix.webp 240w,\n/static/3d1d42e258bf0bcd10b07e2b626101b9/4287c/feature_matrix.webp 475w\"\n              sizes=\"(max-width: 475px) 100vw, 475px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/3d1d42e258bf0bcd10b07e2b626101b9/8ff5a/feature_matrix.png 240w,\n/static/3d1d42e258bf0bcd10b07e2b626101b9/466da/feature_matrix.png 475w\"\n            sizes=\"(max-width: 475px) 100vw, 475px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/3d1d42e258bf0bcd10b07e2b626101b9/466da/feature_matrix.png\"\n            alt=\"feature matrix\"\n            title=\"feature matrix\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Each row of feature matrix represents different features, each column represent each pixel in activation map.</p>\n<p>The feature matrix contains all the inforamtion in style image.</p>\n<p>Another information needs to be known is the relationship between different features. The inner product of the feature matrix could describe the relationship in a clear manner. The new matrix is named as gram matrix.</p>\n<p>In python, the gram matrix could be easily calculated (F is feature matrix):</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def Gram(F):\n    return np.dot(F.T, F) / F.size</code></pre></div>\n<ul>\n<li>The gram matrix is illustrated on the following. Each element in the gram matrix will represent 2 features on the row and column respectively.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 424px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e6590ccc3dbb0c50055b3e011f261194/1cfa9/gram_matrix.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 99.58333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAACoklEQVQ4y41UTW/TQBD1/0WCv4HgjOCIqHqDczlQlOZAS5ISu4ndtFGSpqUhSRt/xHZs73r90Iy7lpumiJFGXs9637x5M2sDNSuKgl2vt/e2bVfMwA7TH6pCQSnyvHyPJPJlBhVKFFI9IfEEsL55t75DZ9JBe9zCxeyCY2KZQC4SyFWKPBRQKKBynbR0o86oDrjJNrB/2zi0DzG9n5YZYwBrAAGAEDvNqANt0xdKIFc5ZC4RRhGur64xGAzg9B3YJz3Ylg3n3IFtl0/HcXZrSGY7NlqtFo6OjtBoNNBsNtHutGGaJqwzC1b/DL1+D71eD5ZlcZzcIEapSODFHpc282YMePD1AL7vM2MpJf7XmOFgNkBr1MKX9mdc3V/xhmmZ/Hz34T1ev33D6yzL8KvbRef0lN+DIMDKdeEHQTkFuilxGmOynMCcdjGcD3mTDsZxjFa7jeOfJxXgarXCcrnkw67rIk1Tjud5zpUY1byhnKssz/hJ2szn86oUz/cZKEkSTkTsbm9vEYYh1us1v5MbRdlmqEBCZTk7GXVsel2OixACm82GNdVGDAlo99gkOYQTQq0lxCLhjX6/zwzokHN+DvHQmL39fXzc+1Rp6D/4Iw2LVEF5AkWmkI0iUPU9u4+bmxtEcYzJZMI6EdMfx8f49v2wAqREFK8Ybl9wJfNKw/F4XMUJMHhgQkZakgTEivaSNOXYk5tCH5RjY1WAURTB9TxuEq1JBur27M+MwXSCzTag1oGs2+1iNBrxmkaDytJNoBGhGDVqvlig0Ww+//uqAxLDTAi4vs+AURSypjTMxIgH3ezixauXEFL8G5Du53A4fKShZqh18zyvYqwrfRbwcnjJt0UblejXmkK3gnTc/ikbz/2tSZ/6AWKhG6AT6wT1HvwFeJQApTUvNGsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/e6590ccc3dbb0c50055b3e011f261194/8ac56/gram_matrix.webp 240w,\n/static/e6590ccc3dbb0c50055b3e011f261194/96b95/gram_matrix.webp 424w\"\n              sizes=\"(max-width: 424px) 100vw, 424px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/e6590ccc3dbb0c50055b3e011f261194/8ff5a/gram_matrix.png 240w,\n/static/e6590ccc3dbb0c50055b3e011f261194/1cfa9/gram_matrix.png 424w\"\n            sizes=\"(max-width: 424px) 100vw, 424px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/e6590ccc3dbb0c50055b3e011f261194/1cfa9/gram_matrix.png\"\n            alt=\"gram matrix\"\n            title=\"gram matrix\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>If one element on the gram matrix is large, the relation between these 2 features will be stronger. For example, in one cell, the row represent the tree, the column is flower, the value is larger means this artist prefers the combination of tree and flower.</p>\n<p>The size of gram matrix is only determined by the number of feature map. In style transfer, the input size is not strictly defined.</p>\n<h1 id=\"training-progress\" style=\"position:relative;\"><a href=\"#training-progress\" aria-label=\"training progress permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Progress</h1>\n<p>The whole process is showed on the following.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 960px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/da2ff99480227e9353883c1846687931/e4611/style_transfer_sketch.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 127.08333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAIAAAC+dZmEAAAACXBIWXMAAC4jAAAuIwF4pT92AAADCElEQVQ4y4WUS4skRRDH+9P6ATyI30DxKnrwoBfRo0LLCrK7uiuM4F4EH7AOjDN2T1dV5/tdkZkRJVlV3Ts6Mxi3zIxfxT+iImIzPWJllEldRvFrNrcoHOk8ZZyI2hutPpvHYKwx+9sofscSp5EoFOSRhkg+n/mHYcKCJWJ2JUksATEQ1fmThMdIBhb+AZgqlMTLKFvMiQgz+A7cgSqsooZIGe9HbmrA30b5Rw491rFdlZTNFdhrwhUmn5Gnh2UT1RwPRHk51lqs6636KyRX6pLwLL7g5h5JCLl4hZAJm5AEoeO7QR6Y7nyyq5tIFPJ/YcwZOAcuspSYWxyu+5v+cs+uL3e/heRWGBB5uhe5FJAydV02hkqZpskGfVQHbnqmOsjj6leb8s1dwavVmo1ZzhNRgsjN0PG/ocz1Q1xhdoqccx5n8943KOcCEIQwwxCcddGYoCxn/sjs8YiE7YcvcK11gZVSMcaWeUpYiuZcHY/Oaqb7TuyMFqLrrRA00RvZiNhggOB9DIFKwZTKmCxjeui9M9oLaZiRzEkZtKZzzjFG51ytFVIyXef2+6x19b4C6P1e39x4I2/5TSd21gjL+WjtKecFtraUErT2SgUpEWAiAmudUl5KaRi3Qyd2gvVeqWjNKpvFTQgBxrFFBjDGJDg1cK2WC94djBU2qAhea66OwxvZLG5SSuMMl1p9CDnnc7c0Idpw1UvLBnUbnfVSprPsIa7VhtlSDOXUBjmGYLTl7DBcMd1JyxQfotFRq3l6Kon5V83tgEiTClWFUuc2aLK1s8ZaH8ccmwNkr22wjqaJ1EihbGjeLIT1o88+f/eDj7/+9inMs7r94ee33vvknQ8//eX1n4uWb55fvP3+l188uaCpkAKKJ9h7v91+9fLZlh2uF9enz188efHs+1evnAuLuh8vftp+9/L11b4VewiPzPN5xdk6ibpe0Z3JY2nZZJsTQHW2RciyrWrKpfNUcL2ZqBJWHkmkR3fYv6xim3vdytMWaB9Jw/nx/+BFx1hJQ1uaFe/u7X8AjG+jPBMdMHcAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/da2ff99480227e9353883c1846687931/8ac56/style_transfer_sketch.webp 240w,\n/static/da2ff99480227e9353883c1846687931/d3be9/style_transfer_sketch.webp 480w,\n/static/da2ff99480227e9353883c1846687931/e46b2/style_transfer_sketch.webp 960w,\n/static/da2ff99480227e9353883c1846687931/94575/style_transfer_sketch.webp 1298w\"\n              sizes=\"(max-width: 960px) 100vw, 960px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/da2ff99480227e9353883c1846687931/8ff5a/style_transfer_sketch.png 240w,\n/static/da2ff99480227e9353883c1846687931/e85cb/style_transfer_sketch.png 480w,\n/static/da2ff99480227e9353883c1846687931/d9199/style_transfer_sketch.png 960w,\n/static/da2ff99480227e9353883c1846687931/e4611/style_transfer_sketch.png 1298w\"\n            sizes=\"(max-width: 960px) 100vw, 960px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/da2ff99480227e9353883c1846687931/d9199/style_transfer_sketch.png\"\n            alt=\"style transfer sketch\"\n            title=\"style transfer sketch\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ul>\n<li>\n<p>In the forward propagation, input the style image and content image into the network, extracted the feature.</p>\n</li>\n<li>\n<p>For each the content image, the content loss is determined as the the sum of all the training images.</p>\n</li>\n<li>\n<p>The cost function is the sum of the style loss and content loss.</p>\n</li>\n<li>\n<p>In the forward propagation, compute the gradient of total loss over the content image. Take the gradient descent until total loss converges.</p>\n</li>\n<li>\n<p>The content loss is a determined value defined by the training images.</p>\n</li>\n</ul>\n<h2 id=\"minimize-the-loss\" style=\"position:relative;\"><a href=\"#minimize-the-loss\" aria-label=\"minimize the loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Minimize the loss</h2>\n<p>The calculation equation of the style loss and content loss is actually the equation of mean square function (MSE).</p>\n<h2 id=\"mse-equation\" style=\"position:relative;\"><a href=\"#mse-equation\" aria-label=\"mse equation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>MSE Equation</h2>\n<p>In the training the loss will be the Mean Square Error (MSE) equation.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 959px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5832a13becfbaed8bbd194efe7dc89f4/9100a/mse.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3oFB/8QAFxABAAMAAAAAAAAAAAAAAAAAAAEhMf/aAAgBAQABBQK2If/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAABD/2gAIAQEABj8Cf//EABsQAAIBBQAAAAAAAAAAAAAAAAARMQEhQYGh/9oACAEBAAE/IU08HWV9DZP/2gAMAwEAAgADAAAAEIPP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGBABAQEBAQAAAAAAAAAAAAAAAREAQTH/2gAIAQEAAT8QASlOHGVK4fJ4whXa05Df/9k='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/5832a13becfbaed8bbd194efe7dc89f4/8ac56/mse.webp 240w,\n/static/5832a13becfbaed8bbd194efe7dc89f4/d3be9/mse.webp 480w,\n/static/5832a13becfbaed8bbd194efe7dc89f4/1bae5/mse.webp 959w\"\n              sizes=\"(max-width: 959px) 100vw, 959px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/5832a13becfbaed8bbd194efe7dc89f4/09b79/mse.jpg 240w,\n/static/5832a13becfbaed8bbd194efe7dc89f4/7cc5e/mse.jpg 480w,\n/static/5832a13becfbaed8bbd194efe7dc89f4/9100a/mse.jpg 959w\"\n            sizes=\"(max-width: 959px) 100vw, 959px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/5832a13becfbaed8bbd194efe7dc89f4/9100a/mse.jpg\"\n            alt=\"mse\"\n            title=\"mse\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h2 id=\"definition-of-total-loss\" style=\"position:relative;\"><a href=\"#definition-of-total-loss\" aria-label=\"definition of total loss permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Definition of Total Loss</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 476px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cf32ed1f1c05cd420539d3c047fa6ba9/f2205/total.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 10.833333333333334%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAYklEQVQI1z2NNw5EMRBC//0v6cK2nHOQO1ZDsR0PEHwpJSilEGOEcw7Ce29yCAHGGFhr4b0n55wx5/z7WmvUWql77/juvSyutRi01vDeYyieHMjIGIOZ+Occ6lIKj4WlI/0fa8uWtZBcxo8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/cf32ed1f1c05cd420539d3c047fa6ba9/8ac56/total.webp 240w,\n/static/cf32ed1f1c05cd420539d3c047fa6ba9/cf53a/total.webp 476w\"\n              sizes=\"(max-width: 476px) 100vw, 476px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/cf32ed1f1c05cd420539d3c047fa6ba9/8ff5a/total.png 240w,\n/static/cf32ed1f1c05cd420539d3c047fa6ba9/f2205/total.png 476w\"\n            sizes=\"(max-width: 476px) 100vw, 476px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/cf32ed1f1c05cd420539d3c047fa6ba9/f2205/total.png\"\n            alt=\"total\"\n            title=\"total\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h2 id=\"training-loss-plot\" style=\"position:relative;\"><a href=\"#training-loss-plot\" aria-label=\"training loss plot permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Loss Plot</h2>\n<ul>\n<li>The loss plot in the training progress. It only takes 4 epoches, approximately 320000 iterations.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 632px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6605827edb897b19e1c54c041bcc7e9b/084e2/loss_plot.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 85.83333333333334%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB7ElEQVQ4y52U7XOiMBCH/f//rPt2H853OwUBAVEERazIWwg8N0lPrq3aXm9ndpJMNs/+NoQdtG1L0zRkWcZHcxyHxWKB67p6HI/HDIdDoijikQ0UMN7vORwOVFWlvSxLPfq+r6FhGOJ5np7btsUuikmygpfsQp7nvSthA0VthND0ruve+SPLLhm277O0beazmU6mgErIoGtbdkmKbG8BHxOoWBVVZEeqcEjgLBhPppim2VeoFaZRgHyg8p5qfe9tRyMlQgjqutYwNR+okMQzaUXdAz+zR/sqiQJrhbFr0ojqS+B1755qKeUrsKxq9r5NU1f/pPBRkl7hZhviPM2Qb0r+DvNGoVpsXZvzJX9fDt3/KVQL17HZbHf9xndKv6vw8pLyZBg01y/2xdN59JQ0UB1WkyRwmBpLCtlec0PXfurdH1exPVBlyItSI86hx3Llk9ZQSo28cf6me2eq5P5PKfMjdbGjuQQEix/sIhcnsJjPf7I0RxjPI2xrhmlOMIwRljlhZc9YWVPM519EO1dDdXNQ5DgOqYozZZHRiozTYc0p9qiPa8RpQ3Fw2VoTyqNPlQaUxzXBcsQhMMj3K8rznmOakiQJA/XoZH9vr1YLieoV6TnH34QEYYy18thGe4JtRCkkQsLbU0LUutv8Bi3dKkuWadR6AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/6605827edb897b19e1c54c041bcc7e9b/8ac56/loss_plot.webp 240w,\n/static/6605827edb897b19e1c54c041bcc7e9b/d3be9/loss_plot.webp 480w,\n/static/6605827edb897b19e1c54c041bcc7e9b/59680/loss_plot.webp 632w\"\n              sizes=\"(max-width: 632px) 100vw, 632px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/6605827edb897b19e1c54c041bcc7e9b/8ff5a/loss_plot.png 240w,\n/static/6605827edb897b19e1c54c041bcc7e9b/e85cb/loss_plot.png 480w,\n/static/6605827edb897b19e1c54c041bcc7e9b/084e2/loss_plot.png 632w\"\n            sizes=\"(max-width: 632px) 100vw, 632px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/6605827edb897b19e1c54c041bcc7e9b/084e2/loss_plot.png\"\n            alt=\"loss plot\"\n            title=\"loss plot\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h1 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h1>\n<ul>\n<li>\n<p>VGG 16 was used to extract the feature and reconstruct images.</p>\n</li>\n<li>\n<p>Training the network takes very long time, a light CNN may be used for time reduction.</p>\n</li>\n<li>\n<p>The tune of parameter α and β to change the output.</p>\n</li>\n</ul>\n<h1 id=\"demo-video\" style=\"position:relative;\"><a href=\"#demo-video\" aria-label=\"demo video permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Demo Video</h1>\n<p>The real-time demo video is here:</p>\n<p><a href=\"https://www.youtube.com/watch?v=5WqvDl81Cj0s\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.youtube.com/watch?v=5WqvDl81Cj0s</a></p>\n<p>Or referenced the gif:</p>\n<p><img src=\"/6517f354dface24f613f24431dc169f8/demo.gif\" alt=\"\"></p>\n<h1 id=\"training-dataset\" style=\"position:relative;\"><a href=\"#training-dataset\" aria-label=\"training dataset permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Dataset</h1>\n<p>The training dataset uses the Microsoft COCO 2014 dataset.</p>\n<p>The size of the dataset is 14GB which has 80000 images in total.</p>\n<h1 id=\"github-page\" style=\"position:relative;\"><a href=\"#github-page\" aria-label=\"github page permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GitHub Page</h1>\n<p><a href=\"https://github.com/jmnie/CS539-ML-18F-Project\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/jmnie/CS539-ML-18F-Project</a></p>\n<h1 id=\"presentation-slides\" style=\"position:relative;\"><a href=\"#presentation-slides\" aria-label=\"presentation slides permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Presentation Slides</h1>\n<p><a href=\"https://prezi.com/kxdwilsggigw/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://prezi.com/kxdwilsggigw/</a></p>","fields":{"slug":"/posts/ml-project//posts/ml-project","tagSlugs":["/tag/tech/","/tag/en/"]},"frontmatter":{"date":"2018-12-09T10:00Z","description":"This webpage is the host page for the WPI CS 539 Group project 2018 Fall.","tags":["tech","EN"],"title":"WPI CS 539 Project - Images Style Transfer Using CNN","socialImage":{"publicURL":"/static/d068b09179d2fac329ed722c8820855c/uol_output.jpg"}}}},"pageContext":{"slug":"/posts/ml-project//posts/ml-project"}},"staticQueryHashes":["251939775","2764776372","401334301"]}